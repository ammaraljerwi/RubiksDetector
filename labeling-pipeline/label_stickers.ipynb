{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1H7Q8y7eTcJPqb8xAxvLA-oQhf_BbE77r","authorship_tag":"ABX9TyM7Xyd24QjPwSXdtmo8wv/x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0pBOsgUeHAd","executionInfo":{"status":"ok","timestamp":1740292848640,"user_tz":360,"elapsed":4423,"user":{"displayName":"Ammar Aljerwi","userId":"12498841310058704773"}},"outputId":"3dd6c37b-6279-49d8-d717-bc14506e8fdd"},"outputs":[{"output_type":"stream","name":"stdout","text":["[(0, 0.76, 0.262, 0.282, 0.274), (0, 0.174, 0.356, 0.282, 0.26), (2, 0.466, 0.312, 0.274, 0.254), (2, 0.802, 0.538, 0.268, 0.246), (2, 0.514, 0.58, 0.268, 0.248), (0, 0.56, 0.832, 0.268, 0.242), (5, 0.224, 0.62, 0.262, 0.234), (4, 0.278, 0.876, 0.258, 0.228), (5, 0.842, 0.794, 0.252, 0.23)]\n","9\n","0 0.76 0.262 0.282 0.274\n","0 0.174 0.356 0.282 0.26\n","2 0.466 0.312 0.274 0.254\n","2 0.802 0.538 0.268 0.246\n","2 0.514 0.58 0.268 0.248\n","0 0.56 0.832 0.268 0.242\n","5 0.224 0.62 0.262 0.234\n","4 0.278 0.876 0.258 0.228\n","5 0.842 0.794 0.252 0.23\n"]}],"source":["import cv2\n","import numpy as np\n","import pickle\n","from pathlib import Path\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","class DummyModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(250 * 250 * 3, 784)\n","        self.fc2 = nn.Linear(784, 256)\n","        self.fc3 = nn.Linear(256, 64)\n","        self.fc4 = nn.Linear(64, 6)\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = self.fc4(x)\n","        return x\n","\n","model = DummyModel()\n","\n","model.load_state_dict(torch.load('drive/MyDrive/new_model_weights.pth', weights_only=True, map_location=torch.device(device)))\n","LABELS = ['green', 'blue', 'orange', 'red', 'yellow', 'white']\n","\n","def pred_from_contours(img: np.ndarray, model: nn.Module) -> list:\n","    # get bounding boxes of contours\n","    detected = img.copy()\n","\n","    contours = get_contours(detected, return_all=False)\n","\n","    yolo_labels = []\n","    if len(contours) == 0:\n","        return detected\n","\n","    for i in range(len(contours)):\n","        r = cv2.boundingRect(contours[i])\n","        x, y, w, h = r\n","        cropped_im = detected[y: y+h, x: x+w]\n","        resized = cv2.resize(cropped_im, (250, 250))\n","        resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n","        resized = torch.from_numpy(resized).float()\n","        resized = resized.unsqueeze(0)\n","        pred = model(resized)\n","        pred = F.softmax(pred, dim=1)\n","        pred = torch.argmax(pred, dim=1)\n","        pred = pred.item()\n","        center_x = x + (w // 2)\n","        center_y = y + (h // 2)\n","        norm_x = center_x / 500\n","        norm_y = center_y / 500\n","        norm_w = w / 500\n","        norm_h = h / 500\n","        yolo_labels.append((pred, norm_x, norm_y, norm_w, norm_h))\n","\n","    return yolo_labels\n","# data dir\n","data_dir = Path('data')\n","\n","# COLOR RANGES\n","\n","# HSV\n","GREEN = [[40, 50, 50], [70, 255, 255]]\n","\n","# HSV\n","BLUE = [[90, 100, 100], [140, 255, 255]]\n","\n","# HSV\n","ORANGE = [[5, 100, 100], [25, 255, 255]]\n","\n","# HSV BUT USE RGB\n","RED = [[115, 100, 100], [140, 255, 255]]\n","\n","# HSV\n","YELLOW = [[25, 110, 110], [50, 255, 255]]\n","\n","# HLS\n","WHITE = [[0, 160, 0], [179, 255, 255]]\n","\n","RANGES = [GREEN, BLUE, ORANGE, RED, YELLOW, WHITE]\n","\n","\n","def detect_color(img: np.ndarray, color_range: list, color_space=cv2.COLOR_BGR2HSV) -> np.ndarray:\n","    image = img.copy()\n","    original = image.copy()\n","\n","    image = cv2.cvtColor(image, color_space)\n","\n","    lower = np.array(color_range[0], dtype=\"uint8\")\n","    upper = np.array(color_range[1], dtype=\"uint8\")\n","\n","    # Create a mask, erode and dilate to remove noise\n","    mask = cv2.inRange(image, lower, upper)\n","    mask = cv2.erode(mask, None, iterations=4)\n","    mask = cv2.dilate(mask, None, iterations=4)\n","\n","    detected = cv2.bitwise_and(original, original, mask=mask)\n","\n","    return detected\n","\n","def filter_color(img: np.ndarray, color_range: list, color_space=cv2.COLOR_BGR2HSV) -> np.ndarray:\n","    detected = detect_color(img, color_range, color_space)\n","\n","    contours, heirarchy = get_contours(detected)\n","\n","    if len(contours) == 0:\n","        return detected\n","\n","    vis = np.zeros((detected.shape[0], detected.shape[1]), dtype=np.uint8)\n","\n","    for i in range(len(heirarchy[0])):\n","        r = cv2.boundingRect(contours[i])\n","        if heirarchy[0][i][2] > -1:\n","            epsilon = 0.1 * cv2.arcLength(contours[i], True)\n","            approx = cv2.approxPolyDP(contours[i], epsilon, True)\n","            if len(approx) == 4:\n","                cv2.drawContours(vis, contours, i, (255, 255, 255), -1)\n","\n","    new_vis = cv2.bitwise_and(detected, detected, mask=vis)\n","\n","    return new_vis\n","\n","def get_contours(img: np.ndarray, thresholds=[50,200], return_all=True, approx_poly=False) -> np.ndarray:\n","    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n","\n","    thresholded = cv2.threshold(cv2.cvtColor(blurred, cv2.COLOR_BGR2GRAY), 25, 255, cv2.THRESH_BINARY)[1]\n","\n","    canny = cv2.Canny(thresholded, thresholds[0], thresholds[1])\n","\n","    contours, heirarchy = cv2.findContours(canny, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n","\n","    if return_all:\n","        return (contours, heirarchy)\n","    else:\n","        kept_contours = []\n","        for i in range(len(heirarchy[0])):\n","            if heirarchy[0][i][2] > -1:\n","                kept_contours.append(contours[i])\n","        kept_contours = sorted(kept_contours, key=cv2.contourArea, reverse=True)\n","        if len(kept_contours) > 9:\n","            kept_contours = kept_contours[:9]\n","\n","        return kept_contours\n","\n","def get_labels(fp):\n","  cropped = cv2.imread(fp)\n","  cropped = cv2.resize(cropped, (500, 500))\n","\n","  final_image = np.zeros_like(cropped)\n","\n","  for i, color in enumerate(RANGES):\n","      if i == 3:\n","          # RED\n","          detected = filter_color(cropped, color, cv2.COLOR_RGB2HSV)\n","      elif i == 5:\n","          # WHITE\n","          detected = filter_color(cropped, color, cv2.COLOR_BGR2HLS)\n","      else:\n","          detected = filter_color(cropped, color)\n","      final_image = cv2.bitwise_or(final_image, detected)\n","\n","\n","  pred = pred_from_contours(final_image, model)\n","  return pred\n","\n","def main():\n","\n","\n","    # cropped = cv2.imread('simulated_cube_dataset/images/train/0313.jpg')\n","    # x1, y1, x2, y2 = [350, 276, 594, 491]\n","    # cropped = cropped[y1:y2, x1:x2]\n","    # cropped = cv2.imread('data/sim2.png')\n","\n","    # cropped = cv2.imread()\n","    # cropped = cv2.resize(cropped, (500, 500))\n","\n","    # final_image = np.zeros_like(cropped)\n","\n","    # for i, color in enumerate(RANGES):\n","    #     if i == 3:\n","    #         # RED\n","    #         detected = filter_color(cropped, color, cv2.COLOR_RGB2HSV)\n","    #     elif i == 5:\n","    #         # WHITE\n","    #         detected = filter_color(cropped, color, cv2.COLOR_BGR2HLS)\n","    #     else:\n","    #         detected = filter_color(cropped, color)\n","    #     final_image = cv2.bitwise_or(final_image, detected)\n","\n","\n","    # pred = pred_from_contours(final_image, model)\n","\n","    pred = get_labels('drive/MyDrive/isolated_cube_dataset/train/images/0015.jpg')\n","\n","    print(pred)\n","    print(len(pred))\n","    for label in pred:\n","      print(\" \".join(map(str, label)))\n","\n","main()\n"]},{"cell_type":"code","source":["image_fp = 'drive/MyDrive/isolated_cube_dataset/{}/images/'\n","label_fp = 'drive/MyDrive/isolated_cube_dataset/{}/labels/'\n","\n","def determine_split(im_processed: int):\n","  '''\n","  takes in integer value, returns split and upper bound\n","  '''\n","  if im_processed < 800:\n","    return 'train', 799\n","  elif 800 <= im_processed < 900:\n","    return 'val', 899\n","  else:\n","    return 'test', 1000\n","\n","def determine_batch(curr, upper_bound):\n","  \"\"\"\n","  checks if 20 will overflow current split, returns proper value\n","  \"\"\"\n","  next_batch = curr + 50\n","  res = upper_bound - next_batch\n","  if res < 0:\n","    return 50 + res\n","  else:\n","    return 50\n","\n","\n","def get_fns(cur, batch):\n","  return [\"{:04d}\".format(i) for i in range(cur, cur + batch + 1)]\n","\n","def write_labels(split, fn, labels):\n","  pt = Path(label_fp.format(split))\n","  path = pt / (fn + '.txt')\n","  with open(path, 'w') as fp:\n","    for label in labels:\n","      res = \" \".join(map(str, label)) + '\\n'\n","      fp.write(res)\n"],"metadata":{"id":"n3_rGr6EhP44","executionInfo":{"status":"ok","timestamp":1740293271132,"user_tz":360,"elapsed":2,"user":{"displayName":"Ammar Aljerwi","userId":"12498841310058704773"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["import os\n","cur = 800\n","\n","while cur <= 1000:\n","  split, upper_bound = determine_split(cur)\n","  batch = determine_batch(cur, upper_bound)\n","  fns = get_fns(cur, batch)\n","  print(f'labeling {cur}-{cur+batch} images in {split} split')\n","  for fn in fns:\n","    fp = image_fp.format(split) + fn +'.jpg'\n","    isExist = os.path.exists(fp)\n","    if not isExist:\n","      continue\n","    labels = get_labels(fp)\n","    write_labels(split, fn, labels)\n","  print(f'labeled')\n","  if not batch:\n","    cur += 1\n","  cur += batch\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_BBaqYKjg7o","executionInfo":{"status":"ok","timestamp":1740295569602,"user_tz":360,"elapsed":105817,"user":{"displayName":"Ammar Aljerwi","userId":"12498841310058704773"}},"outputId":"0226484c-3c38-4125-c3e4-fbfa7236760b"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["labeling 800-850 images in val split\n","labeled\n","labeling 850-899 images in val split\n","labeled\n","labeling 899-899 images in val split\n","labeled\n","labeling 900-950 images in test split\n","labeled\n","labeling 950-1000 images in test split\n","labeled\n","labeling 1000-1000 images in test split\n","labeled\n"]}]}]}